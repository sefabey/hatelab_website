---
title: "hatedash_check"
author: "Sefa Ozalp"
date: "2019-11-03"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

# Introduction

Currently, line graphs in the Hate Speech Dashboards (HSD) for tweets classified with different classifiers are counter-intuitive. For most classifiers, the hate speech classsification results seem disproportunately high. This prompts us to think our HS classifiers perform poorly i.e. too many false positives are outputted. However, we are not sure if this is the case. Put simply, deployment of classifiers and post classification data aggregation might also be the culprits behind unreasonable trends observed.

This document will present a fact checking excercise for the dashboard. Drawing on a dataset collected with COSMOS 1.5 using single 'brexit' keyword for teh whole month of September 2019, I will classify the tweets using same classifiers the dashboard uses, aggregate and visualise the results independent of the dashboard. 


# What Does the HSD Output and What's Wrong With It?


Currently, the HSD visualises six different classification results over time:
1. Posts (all tweets classified as HS by any of the classiers) 
2. Extreme Right Wing Classifier
3. Anti-Muslim Classifier
4. Far-right Classifier
5. Anti-Semitism Classifier
6. Sexual-orientation Classifier

The 'Brexit' data collection is running since April 2019 on the dashboard. When we select 


[this]("../output/html_viz/dynamic_brex_sep19.html")


```{r, eval=T}
htmltools::tags$iframe(title = "My embedded document", src = here::here( "/output/html_viz/dynamic_brexit_sep19.html"))

getwd()


file_test("-f",here::here("../output/html_viz/dynamic_brexit_sep19.html")) 

file_test("-f","/Users/sefaozalp/Documents/Work/hatelab_website/output/html_viz/dynamic_brexit_sep19.html")
```


<iframe src="../output/html_viz/Hate Speech Dashboard.htm" width="100%" height="400" id="igraph" scrolling="no" seamless="seamless" frameBorder="0"> </iframe>
```

# Conclusions

We recognise that the classification performance of each classifier ( as observed by the F-measure) varies across the board. Some classifiers are expected to output more 

We also know that, despite some classifiers classify tweets more le 

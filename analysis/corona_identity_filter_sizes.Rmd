---
title: "corona_identity_filter_sizes"
author: "Sefa Ozalp"
date: "2020-05-01"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

# Introduction

This document explores the ratio of the tweets that need to be classified for the HateDash for the COVID-19 collection. I've randomly selected 2020-04-10 and working with tweets sent that day. Given the number of tweets ingested are very consistent across hours and days within this particular dataset, we can confidently extrapolate findings from this day to the dataset.

```{r, include=FALSE}
knitr::opts_chunk$set( cache = TRUE,
   echo = TRUE, fig.width = 6, autodep = TRUE
)
```


### Load packages
```{r}
library(tidyverse)
library(fs)
library(furrr)

plan(multiprocess)
```

## Data IO
```{r}
dataset <- fs::dir_ls("data/export/") %>% 
  map_df(~ read_rds(.x) %>% 
           select(text, status_id, created_at, screen_name, is_quote, is_retweet, lang, retweet_text,retweet_status_id))
```


# Summary of the Data 

```{r}
dataset %>% 
  skimr::skim()
```
We have ***`r nrow(dataset)`*** tweets sent on this particular day. 

# 1. Filter English Tweets Only
```{r}
dataset_eng <- dataset %>% 
  filter(lang == "en")

nrow(dataset_eng)

dataset_eng %>% 
  skimr::skim()
```
By filtering English tweets only, I reduced the size of tweets ***`r 1-nrow(dataset_eng)/nrow(dataset)`%***. As much as this step might seem redundant as the keywords used for identities are predominantly in English, this is still necessary as many keywords referring to Chinese identity is quite generic (i.e China, Asia, Wuhan etc) and they match with tweets from multiple languages.

# 2. Filter by Identity Related Keywords
Below, I will demonstrate the regex patterns I use to filter tweets for each identity. Note some keywords in Chinese pattern are wrapped in `\\b` [anchor](https://www.regular-expressions.info/wordboundaries.html). This is to prevent pattern matching to detect identity related keywords within parts of the words in tweets but only when they are present as whole words. I also opt to ignore case for pattern matching.

## 2.1 Chinese and Asian
```{r}
regex_chinese_slurs <- c( 'chinese', 'china', 'wuhan', 'oriental','asian',
                                                 'kungflu', 'kung-flu', 'kung flu',
                          'CCPVirus','DeepstateVirus','Depopulation',
                                                 '\\bchapta\\b','\\bchaptas\\b',
                                                 '\\bchigger\\b','\\bchiggers\\b',
                                                 '\\bchink\\b','\\bchinks\\b', 
                                                 '\\bching\\b', '\\bchings\\b',
                                                 '\\bchonky\\b','\\bchork\\b', 
                                                 'ching chong', 
                                                 'chonk','chonks',
                                                 '\\bchang\\b','\\bchangs\\b',
                                                 'chank', 'cheena', 
                                                 '\\bchunk\\b', '\\bchunks\\b',
                                                 'nooger','slanty', 'slit eyes', 'slity eyes', 'yellow skin', 'squint', '\\bgook\\b', '\\bgooks\\b', '\\bnip\\b', '\\bnips\\b', 'chinki', '\\bginks\\b','\\bgink\\b',  'panface', 'pan face','lingling', 'chinazi', '\\bjap\\b','\\bjaps\\b', 'pancake', 'rice eater', 'curry muncher', 'egg head', 'egg-head', 'ironing board', 'coin slot', 'socket face', 'rice ball', 'rice-ball', '\\boreo\\b', 'pumphkin head', 'pumpkin-head', 'burnt rice',  'pan head', 'pan-head', 'bugland', 'chankoro' , 'insectoid' ,'bugmen', 'chingchong', 'chinkistan', 'chinkland', 'chiniggers', 'chinigger') %>% 
  paste0(collapse = "|")

```

```{r}
dataset_eng_chinese <-  dataset_eng %>% 
  filter(str_detect(text, regex(regex_chinese_slurs,ignore_case = TRUE )) )%>% 
  mutate(identity= "chinese")
```

There are ***`r nrow(dataset_eng_chinese)`*** tweets matched with Chinese identity pattern which constitute ***`r nrow(dataset_eng_chinese)/nrow(dataset)*100`*** % of all the tweets in the complete dataset. 

## 2.2 Muslim Identity
```{r}
muslim_keywords <-c( 'muslim', 'muslims', 'islam', 'ramadan','hijab', 'paki', '\\barab\\b', '\\boarabs\\b', 'muzzie', 'burqa', 'burka' ,'Bengali', 'haji', 'hajis', 'hajji', 'sand nigger', 'sand niglet', 'seminigger', 'turk', 'camel fucker', 'derka derka', 'durka durka', 'geitenneuker', 'gerudos', 'jihadi', 'jihadis', 'jihadist', 'jihadists', 'kaffir', 'kaffirs', 'kafir', 'musla', 'muslamic', 'muslimal', 'mussie', 'mussies', 'mussy', 'muzzie', 'muzzies', 'muzzpig', 'muzzpigs', 'muzzrat', 'muzzrats', 'muzzy', 'pisslam', 'sand monkey', 'sand monkeys') %>% 
  paste0(collapse = "|")

muslim_keywords
```

```{r}
dataset_eng_muslim <-  dataset_eng %>% 
  filter(str_detect(text, regex(muslim_keywords,ignore_case = TRUE ))) %>% 
  mutate(identity= "muslim")
```


There are ***`r nrow(dataset_eng_muslim)`*** tweets matched with Muslim identity pattern which constitute ***`r nrow(dataset_eng_muslim)/nrow(dataset)*100`%*** of all the tweets in the complete dataset. 

## 2.3 Jewish Identity
```{r}
jewish_keywords <- read_csv('/Users/sefaozalp/Documents/Work/cyberhate/gofore_dashboard/jewish_identity/jewish_identity_keywords.csv') %>% 
  mutate(value = ifelse(value == '(((', '\\(\\(\\(' ,value )) %>% # escaping (((
  pull(value) %>% 
  paste0(collapse = "|")
jewish_keywords
```

```{r}
dataset_eng_jewish <-  dataset_eng %>% 
  filter(str_detect(text, regex(jewish_keywords,ignore_case = TRUE ))) %>% 
  mutate(identity= "Jewish")
```


There are ***`r nrow(dataset_eng_jewish)`*** tweets matched with Jewish identity pattern which constitute ***`r nrow(dataset_eng_jewish)/nrow(dataset)*100`%*** of all the tweets in the complete dataset. 


## 2.4. Merge all identity subsets

Below, I join all three identity sub-datasets into a large dataset. Note that there will be duplicates. Some tweets might appear in multiple subsets if they match keywords from multiple identity keyword lists, such as the following fake tweet: 'Covid is the result of the chinese and muslims working to destroy USA!!!' I opt not to remove these because in the HateDash will appear in separate identity subsets and separately classified using the classifier pertaining to each identity. 


```{r}
dataset_eng_filtered <- dataset_eng_chinese %>% 
  bind_rows(dataset_eng_muslim) %>% 
  bind_rows(dataset_eng_jewish)
```

When all three identities are considered, there are ***`r nrow(dataset_eng_filtered)`*** tweets matched with any of the identity patterns. This constitutes ***`r nrow(dataset_eng_filtered)/nrow(dataset)*100`%*** of all the tweets in the complete dataset. Roughly, I'd expect to see these figure across all days in the dataset, unless there are temporary spikes. 



# 3. Not Classifying Retweets

Retweets are verbatim duplications of original tweets but they appear as separately in the Twitter data stream. It is redundant to classify retweets separately, given our classification processes are costly. Therefore, it is better to identity original/authored tweets, classify them and get the classification scores for retweets from original tweets. Below, I will demonstrate a brief prototype of how to do this and how much classification this could save. Note that quotes (comment added to retweets) should be treated as original tweets.

```{r}
authored_tweets <- dataset_eng_filtered%>% 
  filter(is.na(retweet_status_id))

nrow(authored_tweets)
```

We can identify original tweets (authored_tweets) by filtering all observations where retweet_status_id is NA. We have ***`r nrow(authored_tweets)`*** authored tweets present in this dataset.

We will need to classify all of these, which is ***`r  nrow(authored_tweets)/nrow(dataset_eng_filtered)*100`%*** of filtered tweets and ***`r nrow(authored_tweets)/nrow(dataset)*100`%*** of the unfiltered dataset (4M tweets per day). 


```{r}
retweets <- dataset_eng_filtered %>% 
  filter(!is.na(retweet_status_id))

nrow(retweets)
```

Using the above logic in reverse, we can also find retweets by filtering the dataset where retweet_status_id is not NA. We have ***`r nrow(retweets)`*** retweets tweets present which is ***`r  nrow(authored_tweets)/nrow(dataset_eng_filtered)*100`%*** of the subset filtered with identity keywords. We do not need to classify all of these as the classification from original tweets (where present) can be recycled. However, in some cases, retweets might be present but the original tweet might not appear in our dataset. This happens relatively rarely, such as when an older tweet is being revived by retweets or we simply don't have the original tweet due to our collection being rate-limited. Although this is rare, HateDash should still account for this and classify retweets which the originals are not present. In such cases, we could classify first RT text and recycle this classification result for later retweets appearing in the dataset.


```{r}
rt_counts <- retweets %>% # counting of RTs of original tweets
  count(retweet_status_id, name = 'RT_count') %>% 
  arrange(desc(RT_count))

rt_counts

rt_counts %>% skimr::skim()
```

Accordingly, in the subset filtered with identity related keywords, ***`r nrow(retweets)`*** retweets are actually replications of ***`r nrow(rt_counts)`*** original/authored tweets. In this subset, the range of retweet counts are ***1*** and ***8994***, with mean RT count being ***10.39***. The histogram of RT counts look like the following:

```{r}
rt_counts %>% 
  ggplot(aes(RT_count))+
  geom_histogram(bins=50)
```

As seen, some tweets get retweeted thousands of times. We can reduce the classification cost by running the classification once on original/authored tweets and use these classification scores for RTs of the original/authored tweet. As this is a database operation this should be quite cheaper than classifying every original/authored tweet and retweet. By running the classification only on original/authored tweets and first RTs where originals are not present, we can reduce the cost of classification greatly.
